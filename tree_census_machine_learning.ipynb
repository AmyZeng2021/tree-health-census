{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Overview - Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of building our machine learning model is to correctly predict a tree's health based on independent variables. We are classifying categorical variables. They are nominal, meaning that they do not have any intrinsic order to them, unlike ordinal variables. To measure our model's success, we are relying on a [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html), which shows the main classification metrics such as precision, recall, and f1-score.\n",
    "\n",
    "We are using [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), and [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). We start by separating the target variable, y or tree health, from the feature variables, X. Next, we split X and y into training and test sets with 70% training and 30% testing. We train the models using training data and gauge their accuracy scores using a classification report.\n",
    "\n",
    "Since the classes are imbalanced, we use under sampling to minimize the size of majority samples so that each class may be represented as equally as possible. The technique we are using is edited nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.7.0-py3-none-any.whl (167 kB)\n",
      "\u001b[K     |████████████████████████████████| 167 kB 12.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (0.14.1)\n",
      "Requirement already satisfied: scikit-learn>=0.23 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (0.23.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (1.18.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn>=0.23->imbalanced-learn->imblearn) (2.1.0)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.7.0 imblearn-0.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import (StratifiedKFold, cross_val_score, GridSearchCV, train_test_split)\n",
    "from sklearn.metrics import (classification_report, confusion_matrix)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# under sampling\n",
    "import imblearn\n",
    "from imblearn.under_sampling import (RandomUnderSampler, EditedNearestNeighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv('tree_ml.csv', index_col=0)\n",
    "\n",
    "tree = data.copy() # save a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree_dbh</th>\n",
       "      <th>curb_loc</th>\n",
       "      <th>health</th>\n",
       "      <th>sidewalk</th>\n",
       "      <th>root_stone</th>\n",
       "      <th>root_grate</th>\n",
       "      <th>root_other</th>\n",
       "      <th>trunk_wire</th>\n",
       "      <th>trnk_light</th>\n",
       "      <th>trnk_other</th>\n",
       "      <th>...</th>\n",
       "      <th>Stew_N</th>\n",
       "      <th>Guard_N</th>\n",
       "      <th>Harmful</th>\n",
       "      <th>Helpful</th>\n",
       "      <th>Unsure</th>\n",
       "      <th>Bronx</th>\n",
       "      <th>Brooklyn</th>\n",
       "      <th>Manhattan</th>\n",
       "      <th>Queens</th>\n",
       "      <th>Staten Island</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Fair</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>Fair</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tree_dbh  curb_loc health  sidewalk  root_stone  root_grate  root_other  \\\n",
       "0         3         1   Fair         0           0           0           0   \n",
       "1        21         1   Fair         1           1           0           0   \n",
       "2         3         1   Good         1           0           0           0   \n",
       "3        10         1   Good         1           1           0           0   \n",
       "4        21         1   Good         1           1           0           0   \n",
       "\n",
       "   trunk_wire  trnk_light  trnk_other  ...  Stew_N  Guard_N  Harmful  Helpful  \\\n",
       "0           0           0           0  ...       1        1        0        0   \n",
       "1           0           0           0  ...       1        1        0        0   \n",
       "2           0           0           0  ...       0        1        0        0   \n",
       "3           0           0           0  ...       1        1        0        0   \n",
       "4           0           0           0  ...       1        1        0        0   \n",
       "\n",
       "   Unsure  Bronx  Brooklyn  Manhattan  Queens  Staten Island  \n",
       "0       0      0         0          0       1              0  \n",
       "1       0      0         0          0       1              0  \n",
       "2       0      0         1          0       0              0  \n",
       "3       0      0         1          0       0              0  \n",
       "4       0      0         1          0       0              0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(651535, 29)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into training and testing sets in a stratified fashion so that resulting sets have the same proportions of classes as the originals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(456074, 28) (456074,)\n",
      "(195461, 28) (195461,)\n"
     ]
    }
   ],
   "source": [
    "X = tree.drop('health', axis=1)\n",
    "y = tree['health']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Training Set:  0.8106623048014138\n",
      "Accuracy Score, Test Set:  0.8106988094811752\n",
      "Classification Report \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fair       0.35      0.02      0.04     28928\n",
      "        Good       0.81      1.00      0.90    158499\n",
      "        Poor       0.00      0.00      0.00      8034\n",
      "\n",
      "    accuracy                           0.81    195461\n",
      "   macro avg       0.39      0.34      0.31    195461\n",
      "weighted avg       0.71      0.81      0.73    195461\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg_pred = logreg.predict(X_test)\n",
    "\n",
    "# accuracy scores\n",
    "print('Accuracy Score, Training Set: ', logreg.score(X_train, y_train))\n",
    "print('Accuracy Score, Test Set: ', logreg.score(X_test, y_test))\n",
    "\n",
    "# classification report\n",
    "print('Classification Report \\n')\n",
    "print(classification_report(y_test, logreg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one takes a long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 72.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 15}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearch\n",
    "knn = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors': [4,15]}\n",
    "\n",
    "clf = GridSearchCV(knn, parameters, cv=5, verbose=1, n_jobs=-1)\n",
    "clf.fit(X, y).best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Training Set:  0.8172314142003272\n",
      "Accuracy Score, Test Set:  0.8088109648472074\n",
      "Classification Report \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fair       0.38      0.07      0.11     28928\n",
      "        Good       0.82      0.98      0.90    158499\n",
      "        Poor       0.43      0.03      0.06      8034\n",
      "\n",
      "    accuracy                           0.81    195461\n",
      "   macro avg       0.54      0.36      0.36    195461\n",
      "weighted avg       0.74      0.81      0.75    195461\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=15)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "\n",
    "# accuracy scoring\n",
    "print('Accuracy Score, Training Set: ', knn.score(X_train, y_train))\n",
    "print('Accuracy Score, Test Set: ', knn.score(X_test, y_test))\n",
    "\n",
    "# classification report\n",
    "print('Classification Report \\n\\n {}'.format(classification_report(y_test, knn_pred)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Training Set: 0.9999824589869188\n",
      "Accuracy Score, Test Set: 0.7409252996761503\n",
      "Classification Report \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fair       0.29      0.30      0.30     28928\n",
      "        Good       0.86      0.85      0.85    158499\n",
      "        Poor       0.18      0.19      0.18      8034\n",
      "\n",
      "    accuracy                           0.74    195461\n",
      "   macro avg       0.44      0.45      0.45    195461\n",
      "weighted avg       0.75      0.74      0.74    195461\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "decision_tree_pred = decision_tree.predict(X_test)\n",
    "\n",
    "# accuracy scores\n",
    "print('Accuracy Score, Training Set:', decision_tree.score(X_train, y_train))\n",
    "print('Accuracy Score, Test Set:', decision_tree.score(X_test, y_test))\n",
    "\n",
    "# classification report\n",
    "print('Classification Report \\n\\n {}'.format(classification_report(y_test, decision_tree_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Training Set: 0.9999298359476751\n",
      "Accuracy Score, Test Set: 0.8061198909245323\n",
      "Classification Report \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fair       0.41      0.19      0.26     28928\n",
      "        Good       0.84      0.95      0.89    158499\n",
      "        Poor       0.34      0.12      0.17      8034\n",
      "\n",
      "    accuracy                           0.81    195461\n",
      "   macro avg       0.53      0.42      0.44    195461\n",
      "weighted avg       0.76      0.81      0.77    195461\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(random_state=42)\n",
    "forest.fit(X_train, y_train)\n",
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "# accuracy scores\n",
    "print('Accuracy Score, Training Set:', forest.score(X_train, y_train))\n",
    "print('Accuracy Score, Test Set:', forest.score(X_test, y_test))\n",
    "\n",
    "# classification report\n",
    "print('Classification Report \\n\\n {}'.format(classification_report(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though accuracy scores are relatively high for our models, they consistently under predict the number of fair and poor trees. Good trees are the vast majority so our next step is to selectively remove those trees until they are about equal in amount to fair and poor trees. By under sampling, we balance the classes so that machine learning models can learn to correctly identify each type of tree. We are using edited nearest neighbors for our under sampling method, which edits the samples based on the edited nearest neighbor method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edited Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset: Counter({'Good': 339374, 'Poor': 26781, 'Fair': 3713})\n",
      "(258907, 28) (258907,)\n",
      "(110961, 28) (110961,)\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "enn = EditedNearestNeighbours()\n",
    "X_enn, y_enn = enn.fit_resample(X, y)\n",
    "\n",
    "print('Resampled dataset:', Counter(y_enn))\n",
    "\n",
    "X_train_enn, X_test_enn, y_train_enn, y_test_enn = train_test_split(X_enn, y_enn, test_size=0.3, random_state=42)\n",
    "\n",
    "print(X_train_enn.shape, y_train_enn.shape)\n",
    "print(X_test_enn.shape, y_test_enn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Training Set:  0.9213771740431893\n",
      "Accuracy Score, Test Set:  0.9215219761898324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fair       0.00      0.00      0.00      1128\n",
      "        Good       0.92      1.00      0.96    101806\n",
      "        Poor       0.68      0.09      0.15      8027\n",
      "\n",
      "    accuracy                           0.92    110961\n",
      "   macro avg       0.54      0.36      0.37    110961\n",
      "weighted avg       0.90      0.92      0.89    110961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(X_train_enn, y_train_enn)\n",
    "logreg_pred = logreg.predict(X_test_enn)\n",
    "\n",
    "# accuracy scores\n",
    "print('Accuracy Score, Training Set: ', logreg.score(X_train_enn, y_train_enn))\n",
    "print('Accuracy Score, Test Set: ', logreg.score(X_test_enn, y_test_enn))\n",
    "\n",
    "# classification report\n",
    "print('Classification Report \\n\\n {}'.format(classification_report(y_test_enn, logreg_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Training Set:  0.9266377502346402\n",
      "Accuracy Score, Test Set:  0.9241174827191536\n",
      "Classification Report \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fair       0.51      0.11      0.18      1128\n",
      "        Good       0.93      1.00      0.96    101806\n",
      "        Poor       0.68      0.12      0.21      8027\n",
      "\n",
      "    accuracy                           0.92    110961\n",
      "   macro avg       0.71      0.41      0.45    110961\n",
      "weighted avg       0.91      0.92      0.90    110961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=15)\n",
    "knn.fit(X_train_enn, y_train_enn)\n",
    "knn_pred = knn.predict(X_test_enn)\n",
    "\n",
    "# accuracy scoring\n",
    "print('Accuracy Score, Training Set: ', knn.score(X_train_enn, y_train_enn))\n",
    "print('Accuracy Score, Test Set: ', knn.score(X_test_enn, y_test_enn))\n",
    "\n",
    "# classification report\n",
    "print('Classification Report \\n\\n {}'.format(classification_report(y_test_enn, knn_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Training Set: 1.0\n",
      "Accuracy Score, Test Set: 0.9088418453330449\n",
      "Classification Report \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fair       0.56      0.61      0.58      1128\n",
      "        Good       0.95      0.95      0.95    101806\n",
      "        Poor       0.40      0.39      0.39      8027\n",
      "\n",
      "    accuracy                           0.91    110961\n",
      "   macro avg       0.64      0.65      0.64    110961\n",
      "weighted avg       0.91      0.91      0.91    110961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree.fit(X_train_enn, y_train_enn)\n",
    "decision_tree_pred = decision_tree.predict(X_test_enn)\n",
    "\n",
    "# accuracy scores\n",
    "print('Accuracy Score, Training Set:', decision_tree.score(X_train_enn, y_train_enn))\n",
    "print('Accuracy Score, Test Set:', decision_tree.score(X_test_enn, y_test_enn))\n",
    "\n",
    "# classification report\n",
    "print('Classification Report \\n\\n {}'.format(classification_report(y_test_enn, decision_tree_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has very high accuracy scores for both training and test sets. It has a high precision rate for good and poor trees and strong recall scores for fair and good trees. The two top performing models both have lower scores for classifying fair trees. Since the accuracy scores are already in the mid-90s, the amount of hyper parameter tuning that we can do is minimal. Thus, to evaluate the model's performance, we are using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.91060373 0.91168519 0.91184741 0.91062955 0.91226529]\n",
      "Average 5-Fold Scores: 0.9114062316350069\n"
     ]
    }
   ],
   "source": [
    "# stratified KFold\n",
    "kf = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "# cross validation\n",
    "tree_score = cross_val_score(decision_tree, X_enn, y_enn, cv=kf)\n",
    "\n",
    "print('Scores: ', tree_score)\n",
    "print(\"Average 5-Fold Scores: {}\".format(np.mean(tree_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score, Training Set: 0.9999613760925737\n",
      "Accuracy Score, Test Set: 0.9415740665639278\n",
      "Classification Report \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fair       0.89      0.65      0.75      1128\n",
      "        Good       0.94      1.00      0.97    101806\n",
      "        Poor       0.83      0.29      0.43      8027\n",
      "\n",
      "    accuracy                           0.94    110961\n",
      "   macro avg       0.89      0.64      0.72    110961\n",
      "weighted avg       0.94      0.94      0.93    110961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(random_state=42)\n",
    "forest.fit(X_train_enn, y_train_enn)\n",
    "forest_pred = forest.predict(X_test_enn)\n",
    "\n",
    "# accuracy scores\n",
    "print('Accuracy Score, Training Set:', forest.score(X_train_enn, y_train_enn))\n",
    "print('Accuracy Score, Test Set:', forest.score(X_test_enn, y_test_enn))\n",
    "\n",
    "# classification report\n",
    "print('Classification Report \\n\\n {}'.format(classification_report(y_test_enn, forest_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has high precision scores overall and strong recall scores for good and fair trees. Since precision and recall have an inverse relationship, we are aiming for high precision scores. The overall accuracy score is at 95% and the difference between the training and test set scores is less than 1%, so our model is not grossly over fitted.\n",
    "\n",
    "To evaluate this model, we are using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.94299348 0.94354773 0.9435207  0.94381734 0.94356049]\n",
      "Average 5-Fold Scores: 0.9434879481380675\n"
     ]
    }
   ],
   "source": [
    "# stratified KFold\n",
    "kf = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "# cross validation\n",
    "forest_score = cross_val_score(forest, X_enn, y_enn, cv=kf)\n",
    "\n",
    "print('Scores: ', forest_score)\n",
    "print(\"Average 5-Fold Scores: {}\".format(np.mean(forest_score)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
